---
layout: post
title:  "Dropout的简单理解"
date:   2016-04-06 15:49:20 +0800
categories: DeepLearning
excerpt: 关于Dropout的简单理解，以及Dropout，DropConnect, denoising的比较
---

### 1. Dropout 的简单理解

#### 1.1 关于过拟合

&emsp;前几天，看了论文`Dropout: A Simple Way to Prevent Neural Networks from Overfitting`, 了解了Dropout，并整理了一下dropout, dropConnect，和Denoising的差异以及作用。  
&emsp;深度神经网络， 有很多非线性层， 这使得模型能够学习到输入比较复杂的关系。但是当训练数据有限时，过于复杂的模型会陷入过拟合的问题当中， 测试集的结果会开始变差。有很多方法可以解决过拟合问题， 即增强模型的泛化能力， L1，L2正则化是通过惩罚权重来改善模型， dropout则是增加模型的稀疏性，在人的神经工作机制中，某一项任务时，起作用的少数神经单元。Dropout的优点：  
* 省时
* 解决过拟合

#### 1.2 dropout 的原理

&emsp;dropout 是指将隐藏层或者输入层的单元以一定的概率随机置零，如果某个节点被置零，则该节点暂时的在该回合中不工作， 对输入输出都没有贡献（只是暂时不工作， 下一个mini batch可能会工作），在下一个mini batch时重新对节点随机置零节点。其网络结构对比如下图所示：  
<p align='center'><img src="/images/dropout/dropout.PNG" height="300" weight="600"></p>  

其中左图（a）是标准的神经网络， 右图（b）是应用了 dropout 的神经网络， 其中被打叉的节点是指被暂时随机置零的节点，假设以0.5的概率置零， 则大概会有一半的节点不工作。假设以（1-p）的概率置零，其前向算法如下所示：

&emsp;![](/images/dropout/ca.PNG) &emsp;&emsp;(1)   
&emsp;![](/images/dropout/ch.PNG) &emsp;&emsp;(2)  
&emsp;![](/images/dropout/r.PNG) &emsp;&emsp;(3)  
&emsp;![](/images/dropout/chh.PNG) &emsp;&emsp;(4)  

&emsp; 由于每次置零的节点不同，使得模型变得多样而又简单，因此能有效的解决过拟合。同时由于很多节点被置零，在解决过拟合问题的同时，dropout还减少了网络的计算量， 节省了模型的训练时间。  

#### 1.3 Dropout, DropConnect, denoising DeepAutoEncoder的比较

* Dropout：以一定的概率对输入层或隐藏层节点置零
* DropConnnect：以一定的概率对权重值随机置零， 减少网络的复杂性
* denoising DeepAutoEncoder：对输入以一定的概率置零，是autoEncoder模型的常用trick。相当于对输入加上噪声，有利于模型学习到输入的更为robust的feature
